# -*- coding: utf-8 -*-
"""Pre-Processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uZLUNAaSzpPlAGXZL39eU9WytAE8Tid1
"""

!pip3 install kagglehub ydata-profiling scikit-learn xgboost imbalanced-learn seaborn matplotlib numpy

"""## Import Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import os

import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from ydata_profiling import ProfileReport

from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif, RFE

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix, classification_report, roc_curve, precision_recall_curve, auc
)

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline

np.random.seed(42)

from google.colab import drive
drive.mount('/content/drive')

"""## Load Dataset"""

df = pd.read_csv('/content/drive/Shareddrives/CMPE 255-Project/CMPE 255- Group Project/WA_Fn-UseC_-Telco-Customer-Churn.csv')

print(f"Dataset shape: {df.shape}")
print(f"\nFirst few rows:")
df.head()

"""## Initial Data Exploration"""

print("=" * 50)
print("DATASET INFORMATION")
print("=" * 50)

print(f"\nShape: {df.shape}")
print(f"\nColumn Names:\n{df.columns.tolist()}")
print(f"\nData Types:\n{df.dtypes}")
print(f"\nMissing Values:\n{df.isnull().sum()}")
print(f"\nDuplicate Rows: {df.duplicated().sum()}")
print(f"\nTarget Variable Distribution:\n{df['Churn'].value_counts()}")
print(f"\nTarget Variable Percentage:\n{df['Churn'].value_counts(normalize=True) * 100}")

"""## Pandas Profiling Report"""

profile = ProfileReport(df, title="Telco Customer Churn - Profiling Report", explorative=True)

profile.to_file("/content/drive/Shareddrives/CMPE 255-Project/CMPE 255- Group Project/telco_churn_profile_report.html")

"""## Data Cleaning - Handle Missing Values"""

print("Missing values per column:")
print(df.isnull().sum())

print("\nData types:")
print(df.dtypes)

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

print(f"\nMissing values in TotalCharges: {df['TotalCharges'].isnull().sum()}")
print(f"\nCustomers with missing TotalCharges and tenure = 0: {df[df['TotalCharges'].isnull()]['tenure'].value_counts()}")

df['TotalCharges'].fillna(0, inplace=True)
print(f"\nRemaining missing values:\n{df.isnull().sum().sum()}")

"""## Data Cleaning - Remove Unnecessary Columns"""

df_clean = df.drop('customerID', axis=1)

print(f"Shape after dropping customerID: {df_clean.shape}")
df_clean.head()

"""## Convert the Senior Citizen column to boolean - as it is non-numeric"""

df_clean['SeniorCitizen'] = df_clean['SeniorCitizen'].apply(lambda x: True if x==0 else False)

"""## Cleanup Data"""

df_clean['MultipleLines'] = df_clean['MultipleLines'].apply(lambda x: 'No' if x=='No phone service' else x)
df_clean['OnlineSecurity'] = df_clean['OnlineSecurity'].apply(lambda x: 'No' if x=='No internet service' else x)
df_clean['OnlineBackup'] = df_clean['OnlineBackup'].apply(lambda x: 'No' if x=='No internet service' else x)
df_clean['DeviceProtection'] = df_clean['DeviceProtection'].apply(lambda x: 'No' if x=='No internet service' else x)
df_clean['TechSupport'] = df_clean['TechSupport'].apply(lambda x: 'No' if x=='No internet service' else x)
df_clean['StreamingTV'] = df_clean['StreamingTV'].apply(lambda x: 'No' if x=='No internet service' else x)
df_clean['StreamingMovies'] = df_clean['StreamingMovies'].apply(lambda x: 'No' if x=='No internet service' else x)

"""##  Exploratory Data Analysis - Target Variable"""

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

df_clean['Churn'].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'])
axes[0].set_title('Churn Distribution', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Churn')
axes[0].set_ylabel('Count')
axes[0].set_xticklabels(['No', 'Yes'], rotation=0)

df_clean['Churn'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', colors=['green', 'red'], startangle=90)
axes[1].set_title('Churn Percentage', fontsize=14, fontweight='bold')
axes[1].set_ylabel('')

plt.tight_layout()
plt.show()

print(f"Class Imbalance Ratio: {df_clean['Churn'].value_counts()['No'] / df_clean['Churn'].value_counts()['Yes']:.2f}:1")

"""## Exploratory Data Analysis - Numerical Features"""

numerical_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns.tolist()
print(f"Numerical columns: {numerical_cols}")

print("\nStatistical Summary:")
print(df_clean[numerical_cols].describe())

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, col in enumerate(numerical_cols):
    axes[idx].hist(df_clean[col], bins=30, edgecolor='black', alpha=0.7)
    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel(col)
    axes[idx].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""## Exploratory Data Analysis - Categorical Features"""

categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()
categorical_cols.remove('Churn')

print(f"Categorical columns: {categorical_cols}")
print(f"\nNumber of categorical features: {len(categorical_cols)}")

for col in categorical_cols:
    print(f"\n{col}: {df_clean[col].nunique()} unique values")
    print(df_clean[col].value_counts())

"""## Exploratory Data Analysis - Correlation Analysis for Numerical Features"""

plt.figure(figsize=(10, 8))
correlation_matrix = df_clean[numerical_cols].corr()
sns.heatmap(
    correlation_matrix,
    annot=True,
    fmt='.2f',
    cmap='coolwarm',
    square=True,
    linewidths=1,
    cbar_kws={"shrink": 0.8}
)
plt.title('Correlation Matrix - Numerical Features', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print("\nStrong correlations (|r| > 0.5):")
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > 0.5:
            print(f"{correlation_matrix.columns[i]} <-> {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]:.3f}")

"""## Exploratory Data Analysis - Churn Analysis by Features"""

fig, axes = plt.subplots(3, 3, figsize=(20, 15))
axes = axes.ravel()

for idx, col in enumerate(categorical_cols[:9]):
    churn_rate = df_clean.groupby(col)['Churn'].value_counts(normalize=True).unstack()
    churn_rate.plot(kind='bar', ax=axes[idx], stacked=False, color=['green', 'red'])
    axes[idx].set_title(f'Churn by {col}', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel(col)
    axes[idx].set_ylabel('Proportion')
    axes[idx].legend(['No Churn', 'Churn'])
    axes[idx].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

"""## Outlier Detection - Boxplots"""

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, col in enumerate(numerical_cols):
    axes[idx].boxplot(df_clean[col])
    axes[idx].set_title(f'Boxplot of {col}', fontsize=12, fontweight='bold')
    axes[idx].set_ylabel(col)

plt.tight_layout()
plt.show()

"""## Outlier Detection - IQR Method"""

def detect_outliers_iqr(df, columns):
    outlier_indices = []

    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        outlier_indices.extend(outliers.index.tolist())

        print(f"\n{col}:")
        print(f" Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}")
        print(f" Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}")
        print(f" Number of outliers: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)")

    return list(set(outlier_indices))

outlier_indices = detect_outliers_iqr(df_clean, numerical_cols)
print(f"\n\nTotal unique rows with outliers: {len(outlier_indices)} ({len(outlier_indices)/len(df_clean)*100:.2f}%)")

"""## Data Preprocessing - Encode Target Variable"""

df_clean['Churn'] = df_clean['Churn'].map({'No': 0, 'Yes': 1})

print("Target variable encoding:")
print(df_clean['Churn'].value_counts())

"""## Data Preprocessing - Encode Categorical Features"""

binary_cols = []
multi_class_cols = []

for col in categorical_cols:
    if df_clean[col].nunique() == 2:
        binary_cols.append(col)
    else:
        multi_class_cols.append(col)

print(f"Binary categorical columns: {binary_cols}")
print(f"Multi-class categorical columns: {multi_class_cols}")

df_encoded = df_clean.copy()

for col in binary_cols:
    unique_vals = df_encoded[col].unique()
    df_encoded[col] = df_encoded[col].map({unique_vals[0]: 0, unique_vals[1]: 1})
    print(f"\n{col}: {unique_vals[0]} -> 0, {unique_vals[1]} -> 1")

df_encoded = pd.get_dummies(df_encoded, columns=multi_class_cols, drop_first=True)

print(f"\nShape after encoding: {df_encoded.shape}")
print(f"New columns: {df_encoded.shape[1] - df_clean.shape[1]} additional columns created")

df_encoded.head()

"""## Feature-Target Separation"""

X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"\nFeature names:\n{X.columns.tolist()}")

"""## Train-Test Split"""

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=0.30,
    random_state=42,
    stratify=y
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.50,
    random_state=42,
    stratify=y_temp
)

print(f"Train size:      {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"Validation size: {X_val.shape[0]}   ({X_val.shape[0]/len(X)*100:.1f}%)")
print(f"Test size:       {X_test.shape[0]}   ({X_test.shape[0]/len(X)*100:.1f}%)")

print("\nTrain distribution:\n", y_train.value_counts())
print("\nValidation distribution:\n", y_val.value_counts())
print("\nTest distribution:\n", y_test.value_counts())

"""## Feature Scaling"""

scaler = StandardScaler()

cols_to_scale = numerical_cols

X_train_scaled = X_train.copy()
X_val_scaled   = X_val.copy()
X_test_scaled  = X_test.copy()

X_train_scaled[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])
X_val_scaled[cols_to_scale]   = scaler.transform(X_val[cols_to_scale])
X_test_scaled[cols_to_scale]  = scaler.transform(X_test[cols_to_scale])

print("Scaling completed!")

"""## Handle Class Imbalance - SMOTE

---


"""

smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

print("Before SMOTE:")
print(y_train.value_counts())

print(f"\nAfter SMOTE:")
print(y_train_balanced.value_counts())

print(f"\nTraining set size increased from {len(X_train_scaled)} to {len(X_train_balanced)}")

"""## Feature Selection - Univariate Selection"""

k = 15
selector = SelectKBest(score_func=f_classif, k=k)

X_train_selected = selector.fit_transform(X_train_balanced, y_train_balanced)

selected_features_mask = selector.get_support()
selected_features = X_train_balanced.columns[selected_features_mask].tolist()

print(f"Top {k} features selected:")
print(selected_features)

X_val_selected = selector.transform(X_val_scaled)
X_test_selected = selector.transform(X_test_scaled)

feature_scores = pd.DataFrame({
    'Feature': X_train_balanced.columns,
    'Score': selector.scores_
}).sort_values('Score', ascending=False)

print(f"\nTop 20 features by score:")
print(feature_scores.head(20))

plt.figure(figsize=(12, 8))
feature_scores_top = feature_scores.head(k)
plt.barh(feature_scores_top['Feature'], feature_scores_top['Score'])
plt.xlabel('Score')
plt.title(f'Top {k} Features - Univariate Selection', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""## Dimensionality Reduction - PCA Analysis"""

pca_full = PCA()
pca_full.fit(X_train_balanced)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(
    range(1, len(pca_full.explained_variance_ratio_) + 1),
    np.cumsum(pca_full.explained_variance_ratio_),
    marker='o',
    linestyle='--'
)
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by PCA Components', fontsize=12, fontweight='bold')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')
plt.axhline(y=0.90, color='g', linestyle='--', label='90% Variance')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.bar(range(1, 21), pca_full.explained_variance_ratio_[:20])
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Variance Explained by First 20 Components', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

n_components_95 = np.argmax(np.cumsum(pca_full.explained_variance_ratio_) >= 0.95) + 1
n_components_90 = np.argmax(np.cumsum(pca_full.explained_variance_ratio_) >= 0.90) + 1

print(f"\nNumber of components for 95% variance: {n_components_95}")
print(f"Number of components for 90% variance: {n_components_90}")

"""## Apply PCA"""

n_components = n_components_95

pca = PCA(n_components=n_components, random_state=42)

X_train_pca = pca.fit_transform(X_train_balanced)

X_val_pca = pca.transform(X_val_scaled)
X_test_pca = pca.transform(X_test_scaled)

print(f"Original feature dimensions: {X_train_balanced.shape[1]}")
print(f"Reduced feature dimensions:  {X_train_pca.shape[1]}")
print(f"Variance explained: {sum(pca.explained_variance_ratio_):.4f}")

"""## Dump the training and testing data"""

save_path = "/content/drive/Shareddrives/CMPE 255-Project/CMPE 255- Group Project/preprocessed-dataset"

os.makedirs(save_path, exist_ok=True)


X_train.to_csv(f"{save_path}/X_train.csv", index=False)
X_val.to_csv(f"{save_path}/X_val.csv", index=False)
X_test.to_csv(f"{save_path}/X_test.csv", index=False)


y_train.to_csv(f"{save_path}/y_train.csv", index=False)
y_val.to_csv(f"{save_path}/y_val.csv", index=False)
y_test.to_csv(f"{save_path}/y_test.csv", index=False)

print("Datasets successfully saved to:", save_path)